# ðŸ’¬ Llama Chatbot with Streamlit

This repository contains a simple chatbot built using Streamlit and powered by the Llama model. The chatbot provides real-time AI-generated responses based on user input, making it an engaging project to showcase AI capabilities with a user-friendly interface.

Features

	â€¢	Streamlit Frontend: A clean, interactive chat interface built with Streamlit.
	â€¢	Llama Model Integration: Uses the Llama model via API for generating chatbot responses.
	â€¢	Chat History: Maintains conversation history using Streamlitâ€™s session state.
	â€¢	Easy-to-Use Interface: Enter a prompt, and the Llama model responds in real-time.

## Project Structure
The project is organized as follows:

.
â”œâ”€â”€ app.py               # Main Streamlit application
â”œâ”€â”€ utils.py             # Utility functions, including the API call to Llama
â”œâ”€â”€ requirements.txt     # Python dependencies for the project
â”œâ”€â”€ README.md            # Project documentation (this file)
â””â”€â”€ __pycache__/         # Cache files generated by Python


## Installation

Follow these steps to set up and run the project locally:

1. Clone the Repository
```
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
```
2. Set Up a Virtual Environment
```
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install Dependencies
```
pip install -r requirements.txt
```

4. Run the Llama Model API

Ensure the Llama model server is running on http://localhost:11434. You can set it up following the official Llama documentation here.

5. Run the Streamlit App
```
streamlit run app.py
```

6. Access the Application

After running the above command, navigate to http://localhost:8501 in your web browser to interact with the chatbot.

## Code Overview

app.py

	â€¢	Title and Interface: Sets up the chatbot interface using Streamlitâ€™s title, caption, and session state for storing chat history.
	â€¢	Message Handling: Displays previous messages and captures new ones from the user input.
	â€¢	API Integration: Calls the Llama model using the call_llama function to get responses for user queries.

utils.py

	â€¢	call_llama(model: str, prompt: str, stream: bool = False) -> Union[Dict, str]:
	â€¢	Sends a POST request to the Llama API.
	â€¢	Takes the model name (llama2), user input (prompt), and optional streaming flag as arguments.
	â€¢	Returns the modelâ€™s response or an error message if the request fails.

Example

Once running, you can interact with the chatbot like this:

```
User: What's the weather today?
Assistant: I currently don't have access to weather data, but I can help with general information or tasks.
```

## Requirements

	â€¢	Python 3.8+
	â€¢	Streamlit
	â€¢	Requests
	â€¢	Running Llama model server (locally or remotely)

## Future Enhancements

	â€¢	Add error handling for failed API calls.
	â€¢	Expand functionality with more AI models.
	â€¢	Add streaming capability for dynamic responses.

## License

This project is licensed under the MIT License - see the LICENSE file for details.